# Linear-Regression-
Write a program (you have to do that indvidually and submit individually! NO group submissions!) that implements a (batch) linear regression using the gradient descent method in Python 3. Use the following gradient calculation: gradient = X N i=1 ~xi(yi − f(~xi)) ~w ← ~w + η · gradient where ~xi is one data point (with N being the size of the data set), η the learning rate, yi is the target output and f(~xi) is the linear function defined as f(~x) = ~w T ~x or equivalently f(~x) = P i wi · xi . Whereas ~w and ~x include the bias/intercept, i.e. w0 (x0 is always 1). All weights should be initialized as 0. Given are the two random example data sets (uploaded in Moodle) named random3 and random4 as csv files. For random3 you have given the solution as well. Your task is to correctly implement the gradient descent method and return for each iteration the weights and sum of squared errors until a given threshold of change in the error is reached1 . The output of your algorithm should be printed onto the console/terminal and should look like this. iteration_number,weight0,weight1,weight2,...,weightN,sum_of_squared_errors.
